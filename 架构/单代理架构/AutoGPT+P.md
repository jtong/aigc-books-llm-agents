# AutoGPT+P

## 概念介绍

AutoGPT+P 是一种结合大模型 (LLM) 和基于可供性 (affordance) 的场景表示来进行任务规划的新系统。以下是对 AutoGPT+P 系统及其工作原理的详细介绍：

### 1. 研究背景与动机

近年来，任务规划领域取得了显著进展，特别是通过结合大模型 (LLMs) 和经典规划算法来提升通用性。然而，这些方法在动态捕捉任务规划问题的初始状态时仍面临挑战。为了解决这个问题，AutoGPT+P 提出了一种将基于可供性 (affordance) 的场景表示与规划系统相结合的方法。

**可供性** 指的是环境和其中的物体对代理可能提供的动作。例如，刀子可以用于切割、抓取或搅拌。通过从基于可供性的场景表示推导规划域，可以实现对任意物体的符号规划。AutoGPT+P 利用这种表示法，从用户用自然语言指定的任务中推导并执行任务计划。

### 2. 系统组成

AutoGPT+P 系统主要分为两个阶段：

#### 2.1 场景感知与可供性提取

在这个阶段，系统通过视觉数据将环境感知为一组对象，并基于对象类别和可供性映射 (OAM) 提取场景可供性。具体步骤如下：

- **对象检测**：使用视觉数据检测场景中的对象，并获得它们的类别和位置。
- **对象-可供性映射 (OAM)**：使用 ChatGPT 自动生成对象类别与其可供性之间的映射关系。这样，每个对象类别都被指派了一组相关的可供性。

### 2.2 基于可供性的任务规划

在这个阶段，系统基于已建立的可供性场景表示和用户指定的目标进行任务规划。具体步骤如下：

- **工具选择**：利用 LLM 从机器人记忆中生成的上下文中选择适当的工具来生成计划。
- **计划生成与执行**：根据选择的工具生成执行任务的计划，并在必要时探索场景或建议替代方案。

### 3. 系统功能与优势

#### 3.1 动态响应

AutoGPT+P 可以在任务执行过程中动态响应环境的变化。例如，当所需的物体不在当前场景中时，系统可以探索环境、提出替代方案或生成部分计划。比如，用户要求一杯牛奶，但场景中没有玻璃杯，系统会建议用杯子代替玻璃杯，从而保证任务的完成。

#### 3.2 人机协作

系统还允许机器人在执行任务时寻求人的帮助。例如，当机器人无法打开牛奶瓶时，它可以请求人类的协助。

#### 3.3 任务规划与执行

AutoGPT+P 系统通过以下几个工具来完成任务规划与执行：

- **计划工具 (Plan Tool)**：生成完整的任务计划。
- **部分计划工具 (Partial Plan Tool)**：在物体缺失的情况下生成部分计划。
- **探索工具 (Explore Tool)**：通过探索新的位置来更新场景表示。
- **替代建议工具 (Suggest Alternative Tool)**：当所需物体缺失时，提供替代物体的建议。

### 4. 实验与验证

在模拟环境和实际机器人 (ARMAR-6 和 ARMAR-DE) 上进行了大量实验，验证了 AutoGPT+P 系统的有效性。实验结果表明，AutoGPT+P 系统在处理复杂任务和缺失物体时表现出色，成功率显著高于现有方法。

在 SayCan 指令集上的评估显示，AutoGPT+P 系统的成功率达到 98%，远超当前最先进的 LLM 规划方法 SayCan 的 81%。此外，在包含 150 个场景的新数据集上的评估中，AutoGPT+P 系统的成功率为 79%。

### 5. 未来工作

未来的工作将包括引入概率元素以提高系统在现实世界中的准确性，改进工具选择过程，以及增强人机交互功能，使系统能够在用户指令不明确时进行澄清，并允许用户在执行过程中修改或终止计划。

## 示例

这里有一个具体的例子来说明 AutoGPT+P 系统的工作流程和功能。

### 任务描述

假设用户给机器人下达了一个任务：“请给我一杯水。” 场景中有几个桌子，桌子上有不同的物体，但没有玻璃杯。以下是详细的步骤和系统的处理过程：

### 1. 场景感知与可供性提取

#### 1.1 对象检测

机器人首先使用其摄像头扫描环境，识别出桌子上的物体。假设检测到的物体如下：
- 桌子1：一个咖啡杯、一盒牛奶
- 桌子2：一个螺丝盒、一瓶喷雾、一块海绵

#### 1.2 对象-可供性映射 (OAM)

系统使用预先训练好的 ChatGPT 模型生成对象和可供性之间的映射。例如：
- 咖啡杯：可供性包括“容纳液体”、“饮用”
- 牛奶盒：可供性包括“倾倒液体”、“打开”、“关闭”
- 海绵：可供性包括“擦拭”

### 2. 基于可供性的任务规划

#### 2.1 工具选择

系统通过 LLM 选择适当的工具来生成计划。对于“请给我一杯水”这个任务，系统会依次使用以下工具：

- **计划工具 (Plan Tool)**：尝试生成一个完整的计划，但发现缺少玻璃杯。
- **替代建议工具 (Suggest Alternative Tool)**：系统识别到场景中没有玻璃杯，于是建议使用咖啡杯作为替代品。

#### 2.2 替代建议

系统使用可供性映射和 ChatGPT 模型提出替代建议：
- 玻璃杯 -> 咖啡杯

#### 2.3 生成计划

系统生成一个包含以下步骤的计划：
1. 探索桌子1，找到咖啡杯和牛奶盒。
2. 向用户确认使用咖啡杯代替玻璃杯。
3. 移动到桌子1，抓住咖啡杯。
4. 打开牛奶盒（假设水被假设成牛奶）。
5. 将牛奶倒入咖啡杯中。
6. 将咖啡杯递给用户。

### 3. 计划执行

机器人根据生成的计划执行以下步骤：

1. **探索**：移动到桌子1，识别出咖啡杯和牛奶盒。
2. **抓取**：使用其机械臂抓住咖啡杯。
3. **打开**：请求用户帮助打开牛奶盒。
4. **倾倒**：将牛奶倒入咖啡杯。
5. **递送**：移动到用户所在位置，将咖啡杯递给用户。

### 4. 反馈与调整

在执行过程中，系统会根据实际情况进行动态调整，例如：
- 如果用户拒绝使用咖啡杯，系统将询问是否有其他替代品或是否需要探索其他位置。
- 如果牛奶盒无法打开，系统会请求用户的帮助。

### 实验结果

在实际实验中，系统能够成功完成任务，将咖啡杯（替代玻璃杯）中的牛奶递给用户。这展示了 AutoGPT+P 系统在处理复杂任务和缺失物体时的动态响应能力和灵活性。

### 总结

通过这个例子，我们可以看到 AutoGPT+P 系统如何通过结合对象检测、可供性映射和基于 LLM 的任务规划工具，来有效地解决用户指定的任务，即使在环境中缺少关键物体时也能提供替代方案并完成任务。

## 当失败的时候，是如何接受反馈和调整的？

当AutoGPT+P系统在执行任务时遇到失败情况时，它会通过一系列反馈和调整机制来提高任务成功率。以下是系统如何处理失败并进行调整的详细描述：

### 1. 识别失败原因

系统在执行计划的每一步时都会监控执行结果，如果某一步失败，系统会尝试识别失败的具体原因。常见的失败原因包括：
- 物体未找到或识别错误
- 物体无法被抓取或操作
- 用户拒绝使用替代物体
- 环境变化导致计划不再适用

### 2. 反馈机制

#### 2.1 自动纠错反馈（基于LLM）

系统会利用LLM（如ChatGPT）来自动纠正错误。以下是该过程的详细步骤：

1. **检测语法错误**：系统会检查自动生成的计划或目标状态中的语法错误。如果发现错误，系统会生成一条描述性错误消息并反馈给LLM，让其纠正错误。例如，如果目标状态语法错误，系统会发送类似“目标状态的语法有误，请修正”的消息。

2. **检测语义错误**：系统会检查生成的目标状态是否有逻辑矛盾。例如，如果目标状态包含“(on apple table) (on apple counter)”，系统会检测到语义错误（苹果不能同时在桌子和柜台上），并反馈给LLM让其纠正。

3. **循环反馈**：LLM收到错误消息后，会生成一个修正后的目标状态或计划，系统会再次检查并反馈，直到没有错误或达到最大反馈次数。

#### 2.2 用户交互反馈

在某些情况下，系统会主动与用户交互以获取反馈：
- **确认替代物体**：如果系统建议使用替代物体（如用咖啡杯代替玻璃杯），系统会询问用户是否接受这个替代方案。如果用户拒绝，系统会尝试其他替代方案或请求进一步的指示。
- **请求帮助**：如果机器人无法完成某个动作（如打开牛奶盒），系统会请求用户的帮助，并在获得帮助后继续执行计划。

### 3. 动态调整计划

根据反馈，系统会对任务计划进行动态调整，以确保任务能够顺利完成。主要调整方法包括：

#### 3.1 重新规划（Re-planning）

当初始计划失败时，系统会重新生成计划。重新规划主要包括以下步骤：
1. **更新场景表示**：根据反馈和新检测到的物体更新场景表示。
2. **生成新目标状态**：根据用户任务和当前场景生成新的目标状态。
3. **调用计划工具**：使用计划工具生成新的任务计划。

#### 3.2 探索新位置

如果需要的物体不在当前场景中，系统会探索新的位置：
1. **选择未探索位置**：选择一个尚未探索的位置，移动机器人到该位置。
2. **检测新物体**：在新位置检测物体并更新场景表示。
3. **重新规划**：基于新的场景表示重新生成计划。

### 实例演示

假设在执行“请给我一杯水”的任务时发生了以下失败情况：

#### 失败情况1：用户拒绝使用咖啡杯

1. **确认替代物体失败**：系统建议用咖啡杯代替玻璃杯，但用户拒绝。
2. **请求用户反馈**：系统询问用户是否有其他可用的杯子或是否需要探索新的位置。
3. **探索新位置**：根据用户反馈，系统决定探索厨房的橱柜。
4. **重新规划**：在橱柜中找到玻璃杯，系统重新生成计划，将水倒入玻璃杯并递给用户。

#### 失败情况2：无法打开牛奶盒

1. **请求用户帮助**：系统尝试打开牛奶盒失败，向用户请求帮助。
2. **用户协助**：用户帮助打开牛奶盒。
3. **继续执行计划**：系统继续执行后续步骤，将牛奶倒入杯中并递给用户。

### 结论

通过自动纠错反馈和用户交互反馈机制，AutoGPT+P系统能够有效识别并处理任务执行过程中遇到的失败情况，通过动态调整计划和请求用户帮助来提高任务成功率。这样，系统不仅能够完成预定任务，还能在复杂和动态的环境中展示出高度的适应性和灵活性。

## 摘录某论文中的实验记录

（编者注：我对AutoGPT+P不是很感兴趣，所以先简单把某论文的实验记录摘录在这里，方便想了解细节的人看，等有时间再整理AutoGPT+P的内容）

### ARMAR-6 和 ARMAR-DE 上的实验描述

这些实验的目的是验证生成的计划在真实机器人上的可执行性。尽管实际执行动作还可以进一步改进，但这些初步实验证明了从符号动作到次符号动作的转换是可行的。实验过程中出现的失败案例大多数是由于对象检测和对象定位错误引起的。为了简化执行过程，我们忽略了一些经常被误检的物体类别。因为对象检测不是本文的主要关注点，所以这样处理是可以理解的。我们计划在未来集成一个更可靠的对象检测和定位器，以减少出错的可能性。当没有出现漏检或误检时，我们的方法能够可靠地生成有效的计划，只有少数失败案例。例如，对于“给我拿一个咖啡杯”这条指令，生成的目标状态是 `and (in-hand coffee_cup0 robot0) (at robot0 human0)`。实验结果如下所示：

#### 任务1：拾取与放置

**任务**：将海绵放在螺丝盒旁边（“把海绵放在桌子上”、“把海绵放在另一张桌子上”）

**位置**：桌子0，桌子1

**初始状态**：
```
at robot0 table1, on sponge0 table0, on tea_packaging0 table0, on tea_packaging1 table0,
on milk_box0 table0, on coffee_cup0 table0, liquid_in milk0 milk_box0, closed milk_box0, on
screw_box0 table1, on spraybottle0 table1, on grease0 table1, on soap0 table1
```

**生成的计划**：
1. 探索桌子0
2. 计划（目标：on sponge0 table1）
   - 抓取：robot0 使用左手从桌子0抓起海绵
   - 移动：robot0 从桌子0移动到桌子1
   - 放置：robot0 使用左手将海绵放在桌子1

**失败案例**：
- “把海绵放在喷雾瓶旁边”（由于误检喷雾瓶）
- “把海绵放在你面前的桌子上”（由于探索前不知道初始位置，错误设定了目标桌子）

#### 任务2：传递物品

**任务**：给我一个玻璃杯（“给我拿一个玻璃杯”、“我要一个玻璃杯”、“递给我一个玻璃杯”）

**位置**：桌子0，人类0

**初始状态**：
```
at robot0 human0, on coffee_cup0 table0, on milk_box0 table0, liquid_in milk0 milk_box0,
closed milk_box0
```

**生成的计划**：
1. 探索桌子0
2. 建议替代物品：玻璃杯 -> 咖啡杯
3. 计划（目标：in-hand coffee_cup0 human0）
   - 抓取：robot0 使用左手从桌子0抓起咖啡杯
   - 移动：robot0 从桌子0移动到人类0
   - 传递：robot0 将咖啡杯递给人类

**失败案例**：
- “递给我一个玻璃杯”（生成的目标状态是 `and (in-hand coffee_cup0 robot0) (at robot0 human0)`）

#### 任务3：倒水

**任务**：我要一杯水（“给我倒一杯水”、“我想喝水”、“我渴了”）

**位置**：桌子0

**初始状态**：
```
at robot0 table0, at human0 table0, on coffee_cup0 table0, on milk_box0 table0, liquid_in
milk0 milk_box0, closed milk_box0
```

**生成的计划**：
1. 建议替代物品：玻璃杯 -> 咖啡杯
2. 建议替代物品：水 -> 牛奶
3. 计划（目标：liquid_in milk0 coffee_cup0）
   - 请求帮助：human0 使用左手打开牛奶盒
   - 抓取：robot0 使用右手从桌子0抓起牛奶盒
   - 倒水：robot0 使用右手将牛奶倒入咖啡杯

**失败案例**：
- “我要一杯水”（LLM 一直调用替代建议工具 coffee cup0，这是完全错误的使用）

#### 任务4：擦拭

**任务**：我把牛奶洒在这张桌子上了（“清理这张桌子”、“擦拭这张桌子”、“这张桌子脏了”）

**位置**：桌子0，桌子1

**初始状态**：
```
at robot0 human0, at human0 table0, on screw_box0 table1, on spraybottle0 table1, on grease0
table1, on soap0 table1, on sponge0 table1
```

**生成的计划**：
1. 探索桌子1
2. 计划（目标：clean table0）
   - 抓取：robot0 使用左手从桌子1抓起海绵
   - 移动：robot0 从桌子1移动到桌子0
   - 擦拭：robot0 使用左手用海绵擦拭桌子0

**失败案例**：
- “我把牛奶洒在桌子上了”（LLM 生成的目标状态是 clean table1，但用户在桌子0旁边）

### 结论

通过这些实验，我们验证了生成的计划可以在机器人上成功执行，并且规划域中的符号表示可以转换为执行技能所需的次符号表示。